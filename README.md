# E-commerce-order-processing-demo

This project demonstrates a real-time data processing pipeline using Apache Kafka and Apache Spark Structured Streaming, fully containerized with Docker Compose.
Streaming e-commerce order events are produced to Kafka, processed in real time by Spark, aggregated using event-time windows, and persisted to Parquet for downstream analytics.

ğŸ“ Architecture

Kafka Producer --> Kafka Topic (orders) --> Spark Structured Streaming --> Parquet Sink (file-based data lake)

ğŸ§° Tech Stack

Apache Kafka â€“ real-time event ingestion

Apache Spark (Structured Streaming) â€“ stream processing & aggregation

PySpark â€“ Spark application code

Docker & Docker Compose â€“ containerized infrastructure

Parquet â€“ columnar storage format

ğŸ“¦ Features

Real-time ingestion from Kafka

Event-time windowed aggregations (1-minute tumbling windows)

Watermarking for late data handling

Fault-tolerant processing using Spark checkpoints

Dual sinks:

Console output for monitoring

Parquet files for batch analytics

ğŸš€ How to Run

1ï¸âƒ£ Start all services

```
docker compose up -d
```

2ï¸âƒ£ Produce sample Kafka events

```
docker exec -it kafka \
  python /producer.py
```

3ï¸âƒ£ Run the Spark streaming job

```
docker exec -it spark-master-kafka-demo \
  /opt/spark/bin/spark-submit \
  --master spark://spark-master-kafka-demo:7077 \
  /opt/spark/app/ecommerce_tracking.py
```

You should see real-time aggregations printed to the console.
