# E-commerce-order-processing-demo

This project demonstrates a real-time data processing pipeline using Apache Kafka and Apache Spark Structured Streaming, fully containerized with Docker Compose.
Streaming e-commerce order events are produced to Kafka, processed in real time by Spark, aggregated using event-time windows, and persisted to Parquet for downstream analytics.

ğŸ“ **Architecture**

Kafka Producer --> Kafka Topic (orders) --> Spark Structured Streaming --> Parquet Sink (file-based data lake)

ğŸ§° **Tech Stack**

Apache Kafka â€“ real-time event ingestion

Apache Spark (Structured Streaming) â€“ stream processing & aggregation

PySpark â€“ Spark application code

Docker & Docker Compose â€“ containerized infrastructure

Parquet â€“ columnar storage format

ğŸ“¦ **Features**

- Real-time ingestion from Kafka

- Event-time windowed aggregations (1-minute tumbling windows)

- Watermarking for late data handling

- Fault-tolerant processing using Spark checkpoints

- Dual sinks:

  Console output for monitoring

  Parquet files for batch analytics

ğŸš€ **How to Run**

1ï¸âƒ£ Start all services

```
docker compose up -d
```

2ï¸âƒ£ Produce sample Kafka events

```
docker exec -it kafka \
  python /producer.py
```

3ï¸âƒ£ Run the Spark streaming job

```
docker exec -it spark-master-kafka-demo \
  /opt/spark/bin/spark-submit \
  --master spark://spark-master-kafka-demo:7077 \
  /opt/spark/app/ecommerce_tracking.py
```

You should see real-time aggregations printed to the console.

ğŸ“Š **Output Data**

Processed events are written as Parquet files:

```
/opt/spark/data/refined_orders/
```

Read the data in batch mode

```
docker exec -it spark-master-kafka-demo \
  /opt/spark/bin/pyspark
```

```
df = spark.read.parquet("/opt/spark/data/refined_orders")
df.show(truncate=False)
```
ğŸ§ **What this simple project demonstrates**

- End-to-end streaming pipeline design

- Spark Structured Streaming semantics

- Kafka offset management & fault tolerance

- Dockerized data infrastructure

- Transition from streaming to batch analytics

ğŸ§¹ **Notes**

- Checkpoint and data directories are not committed to GitHub.

- This project is intended for local development and learning purposes.

ğŸ“Œ **Possible improvements**

- Add schema evolution handling

- Write aggregated output to a database

- Add monitoring with Spark UI & Kafka metrics

- Deploy to cloud storage (S3 / GCS / Azure Blob)

